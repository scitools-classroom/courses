{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course overview\n",
    "\n",
    "Dask is a flexible parallel computing library for Python. It allows dynamic scheduling of tasks for optimising computations and includes collections (extensions of arrays, lists and dataframes) for use with parallel data processing.\n",
    "\n",
    "As of Iris v2.0, dask has been integrated with Iris. This means you can utilise all of the benefits of dask while keeping the fileformat interoperability and other functionality that Iris provides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why does this course exist?\n",
    "\n",
    "This course exists to enable you to use dask and Iris together for more performant, memory efficient data processing by showing some useful patterns for combining dask and Iris.\n",
    "\n",
    "#### Iris/dask integration\n",
    "\n",
    "Iris uses dask to defer data loading and processing by default in a number of situations, including loading and some mathematical, analytical and statistical operations. This course will introduce some patterns for using Iris and dask together to produce your own bespoke data prcessing pipelines that meet your analytical needs while maintaining deferred data.\n",
    "\n",
    "\n",
    "#### Performance\n",
    "\n",
    "A real benefit of dask is parallel data processing by default. This means you will see existing Iris processing paradigms getting faster, and we will show patterns for loading and processing data to take advantage of dask parallel processing so that your bespoke processing pipelines will also run faster.\n",
    "\n",
    "\n",
    "#### Memory usage\n",
    "\n",
    "Data volumes are getting larger and larger, and we are getting to the point where a typical dataset is now larger than the amount of system memory available to process the dataset with. In many cases then, we can no longer just load an entire dataset into memory and perform data processing on the dataset.\n",
    "\n",
    "Dask provides intelligent deferral of loading data from disk into memory (this is often referred to as [out-of-core processing](https://en.wikipedia.org/wiki/External_memory_algorithm)). This dask functionality is available in Iris as well. This means that dask enables Iris to load and perform data processing on data volumes that are significantly larger than system memory available to you without blowing memory. This means data can also be processed faster and with less difficulty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General\n",
    "\n",
    "#### Thread\n",
    "A thread is the simplest unit of computation.\n",
    "\n",
    "#### Process\n",
    "A process is an isolated computation which consists of one or more threads. The threads within a process can be executed concurrently (simultaneously) and have access to the same resources within the process (memory, executable code and variable values). Processes cannot share these resources with other processes. Each process has its own individual address space, while the threads within that process share their address space.\n",
    "\n",
    "#### Graph\n",
    "A way to represent 'things' and their relationships, where a node (circle) is a thing and an edge (line) between nodes is their relationship; e.g. a social network where nodes are people and edges are their friendships.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask\n",
    "\n",
    "#### Dask architecture\n",
    "Below is the basic structure of how the components of dask interact:\n",
    "\n",
    "![](../images/dask_architecture.png)\n",
    "\n",
    "Dask uses a pool of workers to process tasks specified by the scheduler. The scheduler determines the tasks to be done by intelligently traversing the task graph. The task graph is produced dynamically and automatically passed to the scheduler when using dask functionality in your code. In the case of dask distributed, the task graph is submitted to the scheduler by the client.\n",
    "\n",
    "#### Scheduler\n",
    "One of dask’s key benefits is its ability to efficiently schedule tasks to optimize computations. Everything dask does is built on top of “schedulers”. These schedulers take the order of work established by a task graph and find the optimal way to break down and carry out the tasks.\n",
    "\n",
    "Dask has four types of schedulers:\n",
    "- Synchronous: Single thread (good for debugging).\n",
    "        dask.get\n",
    "- Threaded: Utilises a thread pool.\n",
    "        dask.threaded.get\n",
    "- Multiprocessing: Utilises a process pool.\n",
    "        dask.multiprocessing.get\n",
    "- Distributed: Utilises a cluster of distributed machines.\n",
    "        distributed.Client.get\n",
    "\n",
    "#### Worker\n",
    "A worker receives tasks to process from the scheduler and returns output to the scheduler when that processing is finished. A worker can be a thread, a process or a whole machine, depending on the scheduler used.\n",
    "\n",
    "#### Client\n",
    "A client provides the primary point of access to a distributed scheduler and its associated workers. When using a threaded or multiprocessing scheduler interation with the scheduler is handled by the collection or delayed object.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task Graph\n",
    "A graph of tasks (nodes) and the data which is required to pass between them (edges).\n",
    "In the context of dask:\n",
    "- Task = circle\n",
    "- Data = box\n",
    "- Direction of flow = arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "x = 1\n",
    "y = 2\n",
    "z = add(x, y)\n",
    "w = sum([x, y, z])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/task_graph_def_img_1.png)\n",
    "\n",
    "Viewing the relationship between tasks and data can reveal better\\* ways to order the tasks to achieve the same computation.\n",
    "\n",
    "\\* Better can mean in less time or using less memory or both!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dask Graph\n",
    "Dask stores task graphs in a Python dictionary which maps keys to computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsk = {'x': 1,\n",
    "       'y': 2,\n",
    "       'z': (add, 'x', 'y'),\n",
    "       'w': (sum, ['x', 'y', 'z'])\n",
    "      }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask is different from other parallelising libraries in that it uses ordinary Python structures to represent task graphs instead of a specialised API:\n",
    "\n",
    "- `{dicts}`\n",
    "- `(tuples)`\n",
    "- `functions()`\n",
    "- `values`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Terminology\n",
    "\n",
    "Now it is time to test what you have learned! Answer the following questions by assigning each a definition.\n",
    "\n",
    "Definitions:\n",
    "1. A group of threads with shared memory.\n",
    "2. The object which coordinates tasks for workers to process.\n",
    "3. An interface to access the scheduler in a distrubuted parallel computing cluster.\n",
    "4. A representation of the relationsips between a set of interdependent tasks and their data.\n",
    "5. The smallest unit of computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Q: What is a scheduler? (Answer in the cell below.)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Q: What is a thread?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What is a task graph?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What is a process?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What is a client?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Delayed\n",
    "As users we may want to use the functionality of dask to parallelise our own code. The `dask.delayed` interface allows us to do this with a very light API:\n",
    "\n",
    "![](../images/dask_stack.png)\n",
    "\n",
    "Here we see that all of dask is built on top of the `scheduler`. The graph spec layer allows graphs to be generated and interpreted by the scheduler. Dask collections such as `arrays`, `bags` and `dataframes` are built on top of these and provide functionality to use these data types without having to think about how to talk to the `scheduler` underneath. Similarly `delayed` provides us with a lightweight interface to create our own parallelisable code without need to directly talk to the scheduler.\n",
    "\n",
    "The decorator `@delayed` is used to \"dask-ify\" any arbitrary function, which allows said function to be used as part of a task graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "\n",
    "@delayed(pure=True)\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "@delayed(pure=True)\n",
    "def mul(a, b):\n",
    "    return a * b\n",
    "\n",
    "@delayed(pure=True)\n",
    "def inc(a):\n",
    "    return a + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the cells below you will see `z` is a delayed object and the dask methods `visualize()` and `compute()` can be used on them like any other dask collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = add(1, 2)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly `c` is a delayed object which can be visualized and computed; it is slightly more complex than `z` on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = inc(1)\n",
    "b = mul(1, 2)\n",
    "c = add(a, b)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that here we have created objects which are \"lazy\"; when they are created they are not immediately executed. Their execution is `delayed` until a time which we the users (or a scheduler) determine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: 1\n",
    "Run the code in the cell below and then visualize and compute `total`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for x in range(4):\n",
    "    a = inc(1)\n",
    "    b = mul(1, x)\n",
    "    c = add(a, b)\n",
    "    results.append(c)\n",
    "\n",
    "total = delayed(sum, pure=True)(results)\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1:** Visualize total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2:** Compute total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Iris Load with dask\n",
    "Iris is a Python library for analysing and visualising meteorological and oceanographic data sets. A common issue when using Iris to analyse many data files is that loading these files can take a lot of time when done sequentially. A solution is to parallelise the loading of files, which is an ideal task for dask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "We will need a few more libraries to load files using Iris. These include `iris` for loading files to cubes, `os` and `glob` to access files on disk, and `dask.bag` is the type of dask collection we will use to create a task graph of our parallelised load functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris\n",
    "import os\n",
    "import glob\n",
    "import dask.bag as db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we will be loading is monthly northward sea ice velocities from 1890-1899, generated by the Met Office Unified Model and stored in 120 PP files. We will create a list of the filenames, and check the number and names of the files in the list are what we expect, using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(iris.sample_data_path('UM', '*.pp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Earliest: {}\\nLatest: {}'.format(files[0], files[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load multiple files into multiple cubes using `iris.load_cube`\n",
    "\n",
    "When using Iris to load data that data is stored in a structure called a `cube`. Multiple cubes can be stored in a list called a `CubeList`. We can load the data from our 120 PP files into a `CubeList` by mapping our list of filenames (`files`) onto the `iris.load_cube()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubelist = iris.cube.CubeList(map(iris.load_cube, files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cubelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cubelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load multiple files into multiple cubes using `dask.bag`\n",
    "\n",
    "We can parallelise the loading of data from files into cubes by creating a `dask.bag` object which maps our list of filenames onto the function `iris.load_cube()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_o_cubes = db.from_sequence(files).map(iris.load_cube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bag_o_cubes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_o_cubes.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bag_o_cubes` is a delayed (or \"lazy\") object which we can `visualize` as a task graph. The delayed object will not perform the computation of mapping filenames onto `iris.load_cube()` until we give it the command to do so (namely `bag_o_cubes.compute()`). You can see from the task graph (double-click to enlarge) that each file is being loaded into a cube invidually and in parallel.\n",
    "\n",
    "Since we want to generate a `CubeList` like the previous example we can compute `bag_o_cubes` from within our `iris.cube.CubeList()` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_cubelist = iris.cube.CubeList(bag_o_cubes.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bag_cubelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bag_cubelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that part of the beauty of Python is that we do not necessarily need to create four objects (`fp`, `files`,  `bag_o_cubes`, `bag_cubelist`) to get a `CubeList` object but can chain the functions together into one big function call:\n",
    "\n",
    "```python\n",
    "iris.cube.CubeList(db.from_sequence(files).map(iris.load_cube).compute())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load multiple files into single cube using `iris.load`\n",
    "\n",
    "Iris by default will try to merge all input files into the fewest resultant cubes as possible, obeying the rules on what data and metadata can be merged into a single cube. If we know that the result of a load operation will be a single cube we can use `iris.load_cube` to return a single merged cube from the load operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube = iris.load_cube(files)\n",
    "\n",
    "print(cube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to the output of printing a single cube and it is clear that the decade's worth of data has been merged into a single cube with 120 time steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_cube = iris.load_cube(files[0])\n",
    "\n",
    "print(single_cube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load multiple files into single cube using `dask.bag` and `dask.delayed`\n",
    "We can merge the `CubeList` we generate using a `dask.bag` in exactly the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_merged_cube = bag_cubelist.merge_cube()\n",
    "\n",
    "print(bag_merged_cube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However the `merge_cube` step in this does not leverage dask. We can utilise dask in the merge step by decorating a call to `merge_cube` with the `@delayed` decorator. This allows us to integrate it with our task graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def delayed_merge(cubes):\n",
    "    return iris.cube.CubeList(cubes).merge_cube()\n",
    "\n",
    "delayed_merged_cube = delayed_merge(bag_o_cubes)\n",
    "\n",
    "delayed_merged_cube.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again if you expand the task graph (double-click image) you can see that `merge_cube()` is now part of it. Compare this to the task graph of `bag_o_cubes`. What is different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`delayed_merged_cube` is \"lazy\" so we need to call `compute()` to actually execute the task graph and output a merged cube. We can do this within a `print()` call to see that we have merged it successfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(delayed_merged_cube.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Iris Load with Dask and Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often when loading files in Iris we will want to constrain the files on load, in order to avoid loading more data than we need. Loading using constraints has already been covered in the Iris Introduction course and in the Iris documentation. We will now consider some simple examples of loading with constraints and how to use dask in this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = iris.sample_data_path('')\n",
    "# The ?? in files below restricts our search to files with 2 character file extensions.\n",
    "# This excludes files like grib2 and txt.\n",
    "files = glob.glob(os.path.join(filepath, '*.??'))\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constraining by Phenomenon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to constrain data is based on the phenomenon it represents. Below we have added a constraint so only cubes with the name `air_potential_temperature` will be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenom_cubes = iris.load(files, 'air_potential_temperature')\n",
    "phenom_cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to achieve the same thing using dask a few steps are involved. First we create a list of constraints (the same constraint repeated) that can be fed into a map. This list must be the same length as the number of files because the function mapping will iterate through them both sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phen_cstr = ['air_potential_temperature'] * len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a dask bag using the `from_delayed` method. Every file in `files` is passed to `delayed(iris.load)` along with its matching constraint from `phen_cstr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenom_dask_load = db.from_delayed(map(delayed(iris.load), files, phen_cstr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the following you can visualize how this task is chunked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenom_dask_load.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the following will have the same result as the original `iris.load`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenom_dask_cubes = phenom_dask_load.compute()\n",
    "phenom_dask_cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constraining by Coordinate Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to constrain Iris loading is based dimensions and coordinate values. In this example we will only load data with a model level number less than 10. The constraint `mln_cstr` will do this and `mln_cstr_list` will have copies of that restraint which can be used in a mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mln_cstr = iris.Constraint(model_level_number=lambda cell: cell < 10)\n",
    "mln_cstr_list = [mln_cstr] * len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell performs a simple `iris.load` using the constraint we specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mln_cubes = iris.load(files, mln_cstr)\n",
    "mln_cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells a dask bag is made by mapping individual files and the model level number constraint to the delayed `iris.load`. You can visualize and compute this to have the same effect as the `iris.load` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mln_dask_load = db.from_delayed(map(delayed(iris.load), files, mln_cstr_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mln_dask_load.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mln_dask_cubes = mln_dask_load.compute()\n",
    "mln_dask_cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constraining by Time Coordinate Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is quite common for us to filter data based on time. Below we define our time constraint similarly to how we have done previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_cstr = iris.Constraint(time=lambda cell: 1920 <= cell.point.year < 1950)\n",
    "time_cstr_list = [t_cstr] * len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we have used `iris.load` similarly to how we did for the `model_level_number` example. One key difference is that we perform the load with the `iris.FUTURE.context(cell_datetime_objects=True)` option. In Iris v1.x this is necessary to enable cubes to compare their date/time with `datetime` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with iris.FUTURE.context(cell_datetime_objects=True):\n",
    "    time_cubes = iris.load(files, time_cstr)\n",
    "time_cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we've constructed a dask bag like we did in the previous example. You can then visualize and compute this dask bag. However like above, the computation must be done with the `iris.FUTURE.context(cell_datetime_objects=True)` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dask_load = db.from_delayed(map(delayed(iris.load), files, time_cstr_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dask_load.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with iris.FUTURE.context(cell_datetime_objects=True):\n",
    "    time_dask_cubes = time_dask_load.compute()\n",
    "time_dask_cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constraining by Attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also constrain load based on its attributes. For example, in the following cells we will constrain based on the UM version being used. This is very similar to the previous examples except we are using an `iris.AttributeConstraint` rather than an `iris.Constraint` object.\n",
    "\n",
    "First we define the `AttributeConstraint` and the list of its copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "um_cstr = iris.AttributeConstraint(um_version='7.3')\n",
    "um_cstr_list = [um_cstr] * len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we're able to perform a simple iris.load using the `AttributeConstraint`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "um_cubes = iris.load(files, um_cstr)\n",
    "um_cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can map the list of files and `AttributeConstraints` to the delayed `iris.load`. This can then be visualized and computed to the same effect as the `iris.load` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "um_dask_load = db.from_delayed(map(delayed(iris.load), files, um_cstr_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "um_dask_load.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "um_dask_cubes = um_dask_load.compute()\n",
    "um_dask_cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `filter` as a Constraint for Iris Load with Dask\n",
    "\n",
    "We have seen how to load a subset of data by passing a `constraint` to `iris.load` when loading from a list of filenames. We have also seen how to do this by mapping lists of filenames and constraints onto `delayed(iris.load)`, generating a `dask.bag`. Here we will use the `map` and `filter` functionality of `dask.bag` with `iris.load` to subset data using a phenomenon constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we should define what we mean when we talk about `map` and `filter`:\n",
    "\n",
    "- `map(func, seq)` <br> A function which takes a callable (`func`) and a sequence of inputs (`seq`) as arguments. It returns a sequence of outputs which are the result of applying `func` to each of the items in `seq`.\n",
    "\n",
    "\n",
    "- `filter(func, seq)` <br> A function which also takes a callable (`func`) and a sequence of inputs (`seq`) as arguments. `func` must be a callable which returns a Boolean value, i.e. either `True` or `False`. `func` will be applied to every element of `seq` and only if `func` returns `True` will the element of the sequence be included in the output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`filter` sounds very similar to what we were doing when using `iris.load` with a `constraint`. In a similar way to how we generated `bag_o_cubes` we can generate a `dask.bag` by mapping a sequence of filenames onto `iris.load`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_cubes = db.from_sequence(files).map(iris.load)\n",
    "bag_of_cubes.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `filter` function of a `bag_of_cubes` to constrain it based on the phenomenon it represents. To do this we need to define a filter function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenom_filter = lambda cube: cube.name() == 'air_potential_temperature'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we used a `lambda` to define our filter function, which returns `True` if the name of the phenomenon represented by the cube is `'air_potential_temperature'`.\n",
    "\n",
    "`bag_of_cubes` is a nested list of lists, with each list containing the cubes from each file. We can use `flatten` to reduce it to a single list of cubes then apply `filter` to select cubes based on a phenomenon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_bag = bag_of_cubes.flatten().filter(phenom_filter)\n",
    "filtered_bag.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again Python handily allows us to chain together `from_sequence()`, `map()`, `flatten()`, `filter()` and `compute()` into one long line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_filter = lambda cube: cube.name() == 'surface_altitude'\n",
    "\n",
    "db.from_sequence(files).map(iris.load).flatten().filter(alt_filter).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Loading\n",
    "\n",
    "Consider the following code snippets. Despite looking similar, not all of these code snippets will provide parallel file loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell sets up the files needed by the exercise.\n",
    "filepath = iris.sample_data_path('GloSea4')\n",
    "files = glob.glob(os.path.join(filepath, '*.pp'))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlyd = delayed(iris.load)(os.path.join(filepath, '*.pp'))\n",
    "cs1 = db.from_delayed(dlyd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs2 = db.from_sequence(files).map(iris.load_cube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs3 = db.from_delayed(map(delayed(iris.load), files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Print the task graph for each code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** With reference to the task graph produced by each code snippet, state whether or not each code snippet will provide parallel file loading."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** For the code snippets that do not provide parallel file loading, describe why parallel file loading does not happen and suggest a change to the code snippet to enable parallel file loading."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy calculations with Iris and Dask\n",
    "\n",
    "Iris uses Dask to provide deferred data loading and operations :  Both cube data and coordinate values may be *either* Dask or Numpy arrays (i.e. [da.Array](http://dask.pydata.org/en/latest/array.html) or [np.ndarray](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray)).\n",
    "\n",
    "Iris refers to Dask array data as \"lazy\" arrays, and provides a independent, non-Dask-specific API for handling them.\n",
    "For example, the specific cube methods [Cube.lazy_data()](http://scitools.org.uk/iris/docs/v2.0/iris/iris/cube.html?highlight=lazy_data#iris.cube.Cube.lazy_data) and [Cube.has_lazy_data()](http://scitools.org.uk/iris/docs/v2.0/iris/iris/cube.html?highlight=has_lazy_data#iris.cube.Cube.has_lazy_data).  \n",
    "\n",
    "Iris operations that can work on lazy data to produce more lazy data (i.e. a deferred version of the operation) are referred to as \"lazy-preserving\".  These include, in particular :\n",
    "\n",
    "  * taking cube sub-sections, either with [indexing](http://scitools.org.uk/iris/docs/v2.0/userguide/subsetting_a_cube.html#cube-indexing) or [constraint extraction](http://scitools.org.uk/iris/docs/v2.0/userguide/subsetting_a_cube.html#cube-extraction).\n",
    "  * cube [arithmetic operators](http://scitools.org.uk/iris/docs/v2.0/userguide/cube_maths.html#), ( + - \\* / \\** ).\n",
    "  * statistical calculations with certain cube aggregators, such as [MEAN](http://scitools.org.uk/iris/docs/v2.0/iris/iris/analysis.html#iris.analysis.MEAN) and [STD_DEV](http://scitools.org.uk/iris/docs/v2.0/iris/iris/analysis.html#iris.analysis.STD_DEV)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example lazy operation : Wind calculations\n",
    "\n",
    "#### First grab some sample wind vector data\n",
    "This is in cubes of U and V (i.e. eastward and northward winds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = iris.sample_data_path('wind_speed_lake_victoria.pp')\n",
    "u_cube, v_cube = iris.load_cubes(filepath, ('x_wind', 'y_wind'))\n",
    "\n",
    "print('U cube : ')\n",
    "print(u_cube)\n",
    "print('')\n",
    "print('V cube : ')\n",
    "print(v_cube.summary(shorten=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These cubes have lazy data initially, which means they actually contain Dask arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('U cube : lazy = {}'.format(u_cube.has_lazy_data()))\n",
    "print('U cube : raw data content = {}'.format(u_cube.core_data()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris in-built lazy operations : statistics\n",
    "\n",
    "Let's calculate average winds over the region.  \n",
    "This uses the Iris Cube.collapsed() operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_u = u_cube.collapsed(('latitude','longitude'), iris.analysis.MEAN)\n",
    "mean_v = v_cube.collapsed(('latitude','longitude'), iris.analysis.MEAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the input cubes have lazy data, and the MEAN operator is lazy-preserving, the results cubes' data should also be Dask arrays representing deferred calculations -- i.e. the source data has still not been loaded from disk.\n",
    "\n",
    "We will show this in the following exercise ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: check lazy status\n",
    "\n",
    "**1.** Show that the above results have not yet been calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Print the mean wind values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.**  \n",
    "Have the data now been read from disk ?  \n",
    "Do the mean cubes still have lazy content ?  \n",
    "Do the original u/v cubes still have lazy content ?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris in-built lazy operations : cube arithmetic\n",
    "\n",
    "We can easily calculate windspeeds as a vector magnitude.  \n",
    "This can achieved using Iris \"cube arithmetic\" operators on the U and V cubes.  \n",
    "The result is still lazy -- i.e. data has not yet been loaded from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_windspeed = (u_cube * u_cube + v_cube * v_cube) ** 0.5\n",
    "cube_windspeed.rename('wind_speed')\n",
    "\n",
    "print('Windspeed cube:')\n",
    "print(cube_windspeed)\n",
    "print('')\n",
    "print('Windspeed cube : lazy = {}'.format(cube_windspeed.has_lazy_data()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom lazy calculations\n",
    "\n",
    "Suppose though that we want to calculate wind _**directions**_.  \n",
    "\n",
    "This is somewhat harder, as Iris doesn't provide a function for this calculation (an arc-tangent operation).  \n",
    "We can, however, create a deferred calculation with Dask and embed that in a cube.  \n",
    "\n",
    "For example, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.array as da\n",
    "\n",
    "# Calculate arctan(u, v)\n",
    "wind_dirs_array = da.arctan2(u_cube.lazy_data(), v_cube.lazy_data())\n",
    "# Convert to degrees\n",
    "wind_dirs_array = da.rad2deg(wind_dirs_array)\n",
    "\n",
    "# Make a suitable cube + put this into it.\n",
    "wind_dirs_cube = u_cube.copy(data=wind_dirs_array)\n",
    "# Fix up the metadata.\n",
    "wind_dirs_cube.units = 'degrees'\n",
    "wind_dirs_cube.rename('Wind direction')\n",
    "\n",
    "print('Wind directions cube:')\n",
    "print(wind_dirs_cube)\n",
    "print('')\n",
    "print('Some sample data points : ')\n",
    "print(wind_dirs_cube[:2, :3].data)\n",
    "print('')\n",
    "print('Wind directions cube : lazy = {}'.format(wind_dirs_cube.has_lazy_data()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise : working with a custom lazy calculation\n",
    "\n",
    "If you print wind_dirs_cube.data, the cube is no longer lazy\n",
    "\n",
    "**1.** Show that this happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** explain or show why that _*didn't*_ happen when we printed \"Some sample data points\", above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** calculate a _mean_ wind direction statistic.  \n",
    "Show that it behaves like a \"normal\" statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** what happens to laziness when you calculate a *median* wind direction ([iris.analysis.MEDIAN](http://scitools.org.uk/iris/docs/v2.0/iris/iris/analysis.html#iris.analysis.MEDIAN)).\n",
    "\n",
    "Why do you think that is ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.** A more complex custom statistic :  \n",
    "\n",
    "How could you calculate the number of locations where windspeed exceeds a fixed threshold ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
