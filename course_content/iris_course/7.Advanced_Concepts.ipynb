{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris introduction course\n",
    "# 7. Advanced Concepts\n",
    "\n",
    "**Learning Outcome**: by the end of this section, you will be able to utilise some more advanced parts of Iris's functionality.\n",
    "\n",
    "**Duration:** 1 hour\n",
    "\n",
    "**Overview:**<br>\n",
    "7.1 [Load Callbacks](#callbacks)<br>\n",
    "7.2 [Categorised Statistics](#categorical)<br>\n",
    "7.3 [Out-of-core Processing and Lazy Data](#lazy_data)<br>\n",
    "7.4 [Performance Tricks](#performance)<br>\n",
    "7.5 [Summary of the Section](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris\n",
    "import iris.quickplot as qplt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## 7.1 Load Callbacks<a id='callbacks'></a>\n",
    "\n",
    "Sometimes information important for a file load is recorded, not in the file itself, but somewhere else :  typically in the filename or its path.  It is then desirable for such extra information to be added into the cube's metadata.\n",
    "\n",
    "For example, some early GloSea4 model runs recorded the \"ensemble member number\" (or \"realization\" in CF terms) in the filename, but not in actual PP metadata itself. \n",
    "\n",
    "As a result, loading the following data yields two cubes, rather than a single, fully merged, cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = iris.sample_data_path('GloSea4', 'ensemble_00[34].pp')\n",
    "for cube in iris.load(fname, 'surface_temperature'):\n",
    "    print(cube, '\\n', '--' * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To resolve this we can define a function that gets called during the load process. This load callback function must take the following as arguments:\n",
    "\n",
    " * a cube,\n",
    " * a 2D field - either a PP field, a NetCDF variable or a GRIB message depending on the file format being loaded, and\n",
    " * a filename.\n",
    "\n",
    "In our example, some cubes are missing the `realization` coordinate, so we define a function that parses the fname to identify the ensemble member number and includes this value as a `realization` coordinate. We pass this function to load, and the result is a successfully merged cube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def realization_callback(cube, field, fname):\n",
    "    basename = os.path.basename(fname)\n",
    "    if not cube.coords('realization') and basename.startswith('ensemble_'):\n",
    "        cube.add_aux_coord(iris.coords.DimCoord(np.int32(basename[-6:-3]),\n",
    "                                                'realization', units ='1'))\n",
    "\n",
    "print(iris.load_cube(fname, callback=realization_callback))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## 7.2 Categorical Coordinates for Grouped Statistics<a id='categorical'></a>\n",
    "\n",
    "Sometimes we want to be able to categorise data before performing statistical operations on it.\n",
    "\n",
    "For example, we might want to categorise our data by \"day or night time\" or \"season of the year\", so that we can calculate statistics such as a \"daylight maximum\" or \"seasonal mean\" etc. Both of these categorisations would be based on the time coordinate.\n",
    "\n",
    "The <a href='https://scitools-iris.readthedocs.io/en/stable/generated/api/iris/coord_categorisation.html'>iris.coord_categorisation</a> module provides several convenience functions to perform common categorisations on a cube, and a generalised function for making custom ones. \n",
    "\n",
    "Let's load in a cube that represents the monthly air_temperature from April 2006 through to October 2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris.coord_categorisation as coord_cat\n",
    "\n",
    "filename = iris.sample_data_path('ostia_monthly.nc')\n",
    "cube = iris.load_cube(filename, 'surface_temperature')\n",
    "print(cube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a categorisation coordinate to this cube to identify the climatological season (i.e. \"djf\", \"mam\", \"jja\" or \"son\") of each time point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_cat.add_season(cube, 'time', name='clim_season')\n",
    "print(cube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the above print out of the cube, we now have an extra coordinate called `clim_season`.\n",
    "\n",
    "Let's print the coordinate out to take a closer look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cube.coord('clim_season'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a coordinate representing the climatological season, we can use the cube's ``aggregated_by`` method to \"group by and aggregate\" on the season, to produce a new cube that represents the seasonal mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_mean = cube.aggregated_by('clim_season', iris.analysis.MEAN)\n",
    "print(seasonal_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take this further by extracting the winter season, using our newly created coordinate, and producing a plot of the winter zonal mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winter = seasonal_mean.extract(iris.Constraint(clim_season='djf'))\n",
    "\n",
    "qplt.plot(winter.collapsed('latitude', iris.analysis.MEAN))\n",
    "plt.title('Winter zonal mean surface temperature at $\\pm5^{\\circ}$ latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b><font color=\"brown\">Exercise: </font></b>\n",
    "    <p>Calculate the yearly maximum surface_temperature.\n",
    "    <br>Take a look at the documentation for <a href='https://scitools-iris.readthedocs.io/en/stable/generated/api/iris/coord_categorisation.html'>iris.coord_categorisation</a> to work out how to add a coordinate that represent the year to <font face='courier'>cube</font>, then calculate the maximum.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# edit space for user code ...\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE SOLUTION\n",
    "# %load solutions/iris_exercise_7.2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## 7.3 Out-of-core Processing<a id='lazy_data'></a>\n",
    "\n",
    "[Out-of-core processing](https://en.wikipedia.org/wiki/External_memory_algorithm) is a technical term that describes being able to process datasets that are too large to fit in memory at once. In Iris, this functionality is referred to as **lazy data**. It means that you can use Iris to load, process and save datasets that are too large to fit in memory without running out of memory. This is achieved by loading only the dataset's metadata and not the data array, unless this is specifically requested.\n",
    "\n",
    "To determine whether your cube has lazy data you can use the `has_lazy_data` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = iris.sample_data_path('air_temp.pp')\n",
    "cube = iris.load_cube(fname)\n",
    "print(cube.has_lazy_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Iris tries to maintain lazy data as much as possible.  Many Iris operations, like the arithmetic and statistics we have seen, can be implemented by a \"lazy algorithm\", which produces a representation of the calculation without actually performing it (initially).\n",
    "\n",
    "This has two key benefits :\n",
    "\n",
    "  1. we can easily form a large overall result but then only extract small selected portions; and\n",
    "  2. where a result is smaller than the source data (like a statistic), it can be calculated in sections without ever loading all the original data (which may exceed available memory).\n",
    "\n",
    "We refer to the operation of loading a cube's lazy data as 'realising' the cube's data. A cube's lazy data will only be loaded in a limited number of cases, including:\n",
    "\n",
    "* when the user directly requests the cube's data using `cube.data`,\n",
    "* when there is no lazy data processing algorithm available to perform the requested data processing, such as for peak finding, and\n",
    "* where actual data values are necessary, such as for cube plotting.\n",
    "\n",
    "See : [Iris Userguide : \"Real and Lazy Data\"](https://scitools-iris.readthedocs.io/en/stable/userguide/real_and_lazy_data.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.data\n",
    "print(cube.has_lazy_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we have triggered the data to be loaded into memory by calling `cube.data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b><font color=\"brown\">Exercise: </font></b>\n",
    "    <p>Load the <font face='courier'>sea_water_potential_temperature</font> cube from the file <font face='courier'>iris.sample_data_path('atlantic_profiles.nc')</font>. Does this cube have lazy data?<br>Calculate the mean over the depth coordinate. Does the cube still have lazy data?<br>Create a blockplot (pcolormesh) of the resulting 2D cube. Does the cube still have lazy data?</p> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# edit space for user code ...\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE SOLUTION\n",
    "# %load solutions/iris_exercise_7.3a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## 7.4 Performance Tricks<a id='performance'></a>\n",
    "\n",
    "This section details a few common tricks to improve the performance of your Iris code:\n",
    "\n",
    " * Data loading.\n",
    " * Load once, extract many times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Use of Deferred Loading of Data\n",
    "\n",
    "Sometimes it makes sense to load data before doing operations, other times it makes sense to do data reduction before loading.\n",
    "\n",
    "We define a simple function the applies some processing to the cube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zonal_sum(cube):\n",
    "    \"\"\"\n",
    "    A really silly function to calculate the sum of the grid_longitude\n",
    "    dimension.\n",
    "    Don't use this in real life, instead consider doing:\n",
    "    \n",
    "        cube.collapsed('grid_longitude', iris.analysis.SUM)\n",
    "    \n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for i, _ in enumerate(cube.coord('grid_longitude')):\n",
    "        total += cube[..., i].data\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try loading in our cube, and then applying the `zonal_sum` function to the cube whilst it still has lazy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1\n",
    "fname = iris.sample_data_path('uk_hires.pp')\n",
    "pt = iris.load_cube(fname, 'air_potential_temperature')\n",
    "result = zonal_sum(pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try doing the same thing, but this time we tell Iris to load the data into memory prior to applying the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1\n",
    "fname = iris.sample_data_path('uk_hires.pp')\n",
    "pt = iris.load_cube(fname, 'air_potential_temperature')\n",
    "pt.data\n",
    "result = zonal_sum(pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, loading all the data upfront was much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Once, Extract Many Times\n",
    "\n",
    "Iris loading can be slow, particularly if the format stores 2d fields of a conceptually higher dimensional dataset, as is the case with GRIB and PP. \n",
    "\n",
    "To maximise load speed and avoid unncecessary processing, it is worth constraining the fields that are of interest *at load time*, but there is no caching, so loading a file twice will be twice as slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare loading data on a select number of model levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = iris.sample_data_path('uk_hires.pp')\n",
    "model_levels = [1, 4, 7, 16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's constrain to our chosen model levels at load time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5\n",
    "for model_level in model_levels:\n",
    "    pt = iris.load_cube(fname,\n",
    "                        iris.Constraint('air_potential_temperature',\n",
    "                                        model_level_number=model_level))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's first load in the file, then extract each of our model levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 5\n",
    "cubes = iris.load(fname)\n",
    "for model_level in model_levels:\n",
    "    pt = cubes.extract(iris.Constraint('air_potential_temperature',\n",
    "                                       model_level_number=model_level))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, by loading a cube before applying any of the constraints or extractions the process will be shorter when compared to applying the constraints and extraction operations during the loading process.
    "\n",
    "This is because when applying the constraints at load time it must check every 2d field in the file to see if they match the constraints, whereas by loading the file in first it merges the 2d fields into a cube and so when asked to extract a certain section, it can index through the cube and return without having to check any other section of the file to see if it matches the constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For files with lots of different phenomenon this can be improved further by loading only the phenomenon (and in this case just the model levels of interest):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10\n",
    "cube = iris.load(fname,\n",
    "                 iris.Constraint('air_potential_temperature',\n",
    "                                 model_level_number=model_levels))\n",
    "for model_level in model_levels:\n",
    "    pt = cube.extract(iris.Constraint(model_level_number=model_level))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Section Summary: Advanced Concepts<a id='summary'></a>\n",
    "\n",
    "In this section we learnt:\n",
    "* load callbacks can be used to capture additional metadata during loading\n",
    "* special facilities are provided for performing categorical statistics\n",
    "* Iris uses lazy data and out-of-core-processing to handle data that it too large to fit into memory\n",
    "* lazy loading can be used to enhance code performance\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
