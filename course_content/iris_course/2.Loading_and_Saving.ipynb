{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris introduction course\n",
    "# 2. Loading and Saving\n",
    "\n",
    "**Learning Outcome**: by the end of this section, you will be able to use Iris to load datasets from disk as Iris cubes and save Iris cubes back to disk.\n",
    "\n",
    "**Duration:** 1 hour\n",
    "\n",
    "**Overview:**<br>\n",
    "2.1 [Iris Load Functions](#iris_load_functions)<br>\n",
    "2.2 [Saving Cubes](#saving)<br>\n",
    "2.3 [Out-of-core Processing](#out_of_core_processing)<br>\n",
    "2.4 [Exercise](#exercise_2)<br>\n",
    "2.5 [Summary of the Section](#summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## 2.1 Iris Load Functions<a id='iris_load_functions'></a>\n",
    "\n",
    "There are three main load functions in Iris: ``load``, ``load_cube`` and ``load_cubes``.\n",
    "\n",
    "1. **load** is a general purpose loading function. Typically this is where all data analysis will start, before more loading is refined with the more controlled loading from the other two functions.\n",
    "2. **load_cube** returns a single cube from the given source(s) and constraint. There will be exactly one cube, or an exception will be raised.\n",
    "3. **load_cubes** returns a cubelist of cubes from the given sources(s) and constraint(s). There will be exactly one cube per constraint, or an exception will be raised.\n",
    "\n",
    "\n",
    "Note: ``load_cube`` is a special case of ``load``. \n",
    "\n",
    "Let's compare the result of calling `load` and `load_cube`. We start by selecting the `air_temp.pp` file from Iris' sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = iris.sample_data_path('air_temp.pp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we give this filepath to `load`, we see that Iris returns a cubelist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = iris.load(fname)\n",
    "print(type(c1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If instead we give this filepath to `iris.load_cube` we see that Iris a cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2 = iris.load_cube(fname)\n",
    "print(type(c2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare the first cube in the cubelist returned by calling `load` and the cube returned by calling `load_cube` we see that they are the equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1[0] == c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b><font color='brown'>Exercise: </font></b>Try loading in the `uk_hires.pp` from Iris' sample data, first with iris.load then iris.load_cube. Why does iris.load_cube fail if you only supply the filepath?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# edit space for user code ...\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## 2.2 Saving Cubes<a id='saving'></a>\n",
    "\n",
    "The [iris.save](https://scitools.org.uk/iris/docs/latest/iris/iris.html#iris.save) function provides a convenient interface to save Cube and CubeList instances.\n",
    "\n",
    "Below we load the `uk_hires.pp` file from Iris' provided sample data which returns a cubelist of the cubes that were produced from that file. We then save this cubelist out to netcdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = iris.sample_data_path('uk_hires.pp')\n",
    "cubes = iris.load(fname)\n",
    "iris.save(cubes, 'saved_cubes.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the ncdump to see what Iris saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ncdump -h saved_cubes.nc | head -n 20\n",
    "!rm saved_cubes.nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra keywords can be passed to specific fileformat savers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b><font color='brown'>Exercise: </font></b>Take a look at the above link to the `iris.save` documentation to see what fileformats iris supports saving to.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## 2.3 Out-of-core Processing<a id='out_of_core_processing'></a>\n",
    "\n",
    "[Out-of-core processing](https://en.wikipedia.org/wiki/External_memory_algorithm) is a technical term that describes being able to process datasets that are too large to fit in memory at once. In Iris, this functionality is referred to as **lazy data**. It means that you can use Iris to load, process and save datasets that are too large to fit in memory without running out of memory. This is achieved by loading only the dataset's metadata and not the data array, unless this is specifically requested.\n",
    "\n",
    "To determine whether your cube has lazy data you can use the `has_lazy_data` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = iris.sample_data_path('air_temp.pp')\n",
    "cube = iris.load_cube(fname)\n",
    "print(cube.has_lazy_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris tries to maintain lazy data as much as possible. We refer to the operation of loading a cube's lazy data as 'realising' the cube's data. A cube's lazy data will only be loaded in a limited number of cases, including:\n",
    "\n",
    "* when the user directly requests the cube's data using `cube.data`,\n",
    "* when there is no lazy data processing algorithm available to perform the requested data processing, such as for peak finding, and\n",
    "* where actual data values are necessary, such as for cube plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.data\n",
    "print(cube.has_lazy_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we have triggered the data to be loaded into memory by calling `cube.data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## 2.4 Exercise 2<a id='exercise_2'></a>\n",
    "\n",
    "1\\. Load the file in `iris.sample_data_path('atlantic_profiles.nc')`, as in Exercise 1, but this time use `iris.load_cube` to load in the `sea_water_potential_temperature` cube only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for user code ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Check whether this cube has lazy data? Print the minimum, maximum, mean and standard deviation of the cube's data. Does the cube still have lazy data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for user code ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Go to the Iris reference documentation for ``iris.save``. What keywords are accepted to ``iris.save`` when saving a PP file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for user code ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise Solutions: \n",
    "Once you are happy with your answers, you can check the solutions by runnning the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE SOLUTION\n",
    "# %load solutions/iris_exercise_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## 2.5 Section Summary : Loading and Saving<a id='summary'></a>\n",
    "\n",
    "In this section we learnt:\n",
    "* Iris has different loading functions for different purposes, for example `iris.load` should be used for exploratory data analysis\n",
    "* Iris supports saving to three fileformats: netCDF, GRIB2 and PP\n",
    "* Iris uses lazy data and out-of-core-processing to handle data that it too large to fit into memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
